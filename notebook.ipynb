{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69856482",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agentic RAG ( HyDE & )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d60c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61cfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_17484\\2473198713.py:13: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_17484\\2473198713.py:13: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_17484\\2473198713.py:13: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import faithfulness, answer_relevancy, context_precision\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from gpt4all import GPT4All\n",
    "import pickle\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.language_models import LLM\n",
    "\n",
    "from langchain_core.outputs import LLMResult\n",
    "from datasets import Dataset\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e5b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd9601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_17484\\1906549180.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_17484\\1906549180.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19 pages\n",
      "Created 143 chunks\n",
      "Number of vectors in FAISS: 143\n",
      "Index saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# ── Embeddings ──\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:8b\", # Match the pulled name\n",
    "    temperature=0.7,\n",
    "    num_predict=100 \n",
    ")\n",
    "\n",
    "# ── Load document ──\n",
    "try:\n",
    "    loader = PyPDFLoader(\"https://arxiv.org/pdf/2005.11401.pdf\")\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} pages\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load PDF:\", e)\n",
    "    raise\n",
    "\n",
    "# ── Split ──\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,       # ↑ increased\n",
    "    chunk_overlap=200     # ↑ increased\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "texts = [chunk.page_content for chunk in chunks]\n",
    "print(f\"Created {len(texts)} chunks\")\n",
    "\n",
    "# ── Embed ──\n",
    "embedding_vectors = embeddings.embed_documents(texts)\n",
    "embeddings_np = np.array(embedding_vectors).astype('float32')\n",
    "\n",
    "# ── FAISS index ──\n",
    "dimension = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "print(f\"Number of vectors in FAISS: {index.ntotal}\")\n",
    "\n",
    "# ── Save ──\n",
    "faiss.write_index(index, \"faiss_index.index\")\n",
    "with open(\"texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(texts, f)\n",
    "\n",
    "print(\"Index saved successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "# ── One-time BM25 index creation (do this after loading texts) ──\n",
    "tokenized_texts = [text.lower().split() for text in texts]  # simple tokenization\n",
    "bm25 = BM25Okapi(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c359e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "#llm_logic = Ollama(model=\"llama3.1:8b\", temperature=0, num_predict=10)\n",
    "\n",
    "# This one is for the actual RAG answering and rewriting\n",
    "#llm_assistant = Ollama(model=\"llama3.1:8b\", temperature=0.7, num_predict=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8da924",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Global in-memory cache (query embedding → answer)\n",
    "# You can later save this to disk with pickle/json\n",
    "cache: Dict[tuple, str] = {}           # tuple because numpy arrays are not hashable\n",
    "SIMILARITY_THRESHOLD = 0.95          # tune between 0.90–0.96\n",
    "\n",
    "def get_cached_response(query: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Returns cached answer if very similar query was asked before.\n",
    "    Returns None if no good match.\n",
    "    \"\"\"\n",
    "    # Embed current query\n",
    "    q_vec = np.array(embeddings.embed_query(query))\n",
    "\n",
    "    for cached_vec_tuple, cached_answer in cache.items():\n",
    "        cached_vec = np.array(cached_vec_tuple)\n",
    "        similarity = np.dot(q_vec, cached_vec) / (\n",
    "            np.linalg.norm(q_vec) * np.linalg.norm(cached_vec) + 1e-10\n",
    "        )\n",
    "        if similarity >= SIMILARITY_THRESHOLD:\n",
    "            print(f\"Cache HIT! Similarity = {similarity:.4f}\")\n",
    "            return cached_answer\n",
    "\n",
    "    print(\"Cache miss → running full pipeline\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_to_cache(query: str, answer: str):\n",
    "    \"\"\"\n",
    "    Save the query embedding and its answer to cache.\n",
    "    \"\"\"\n",
    "    q_vec = embeddings.embed_query(query)\n",
    "    # Convert to tuple so it can be used as dict key\n",
    "    cache[tuple(q_vec)] = answer\n",
    "    print(\"Saved to cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f45789",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "index = faiss.read_index('faiss_index.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3053dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_answer_good(question: str, answer: str, context: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Ask the LLM to judge if its own answer is good.\n",
    "    Returns (is_good: bool, reason: str)\n",
    "    \"\"\"\n",
    "    judge_prompt = f\"\"\"You are a very strict quality checker for RAG answers.\n",
    "\n",
    "Evaluate this answer for the given question and context:\n",
    "- Is it complete? (covers main points)\n",
    "- Is it faithful? (only uses info from context, no added facts)\n",
    "- Is it relevant, clear, and helpful?\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "Context (truncated): {context[:1500]}...\n",
    "\n",
    "Reply with exactly this format:\n",
    "YES or NO\n",
    "One short sentence reason.\n",
    "\n",
    "Examples:\n",
    "YES - Answer is complete, faithful, and directly answers the question.\n",
    "NO - Answer adds information not present in the context.\n",
    "\n",
    "Your reply:\"\"\"\n",
    "\n",
    "    judge_response = llm.invoke(judge_prompt).strip()\n",
    "\n",
    "    # Simple parsing\n",
    "    lines = judge_response.split(\"\\n\", 1)\n",
    "    verdict = lines[0].strip().upper()\n",
    "    reason = lines[1].strip() if len(lines) > 1 else \"No reason given\"\n",
    "\n",
    "    is_good = \"YES\" in verdict\n",
    "    print(f\"Self-check verdict: {verdict} - {reason}\")\n",
    "\n",
    "    return is_good, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0b4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss → running full pipeline\n",
      "RETRIEVE\n",
      "Rewritten query: What is the concept of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs), and how does it function?\n",
      "Cache miss → running full pipeline\n",
      "\n",
      "Final chunks after hybrid + reranking:\n",
      "1. explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
      "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
      "ory for language generation. We introduce RAG...\n",
      "2. pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\n",
      "rather than related training pairs. This said, RAG techniques may work well in these settings, and\n",
      "...\n",
      "3. Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
      "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
      "[33, 59, 39], we a...\n",
      "4. We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\n",
      "non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\n",
      "retriever. ...\n",
      "Self-check verdict: HERE ARE MY EVALUATIONS: - **Is it complete?**\n",
      "YES. The answer covers the main points about RAG models.\n",
      "\n",
      "**Is it faithful?**\n",
      "YES. The answer only uses information from the provided context and does not add any extra facts.\n",
      "\n",
      "**Is it relevant, clear, and helpful?**\n",
      "NO. While the answer is brief, it's a bit vague and doesn't directly address how to explain RAG in terms of language models; instead, it describes what RAG models are. A\n",
      "Retry triggered: **Is it complete?**\n",
      "YES. The answer covers the main points about RAG models.\n",
      "\n",
      "**Is it faithful?**\n",
      "YES. The answer only uses information from the provided context and does not add any extra facts.\n",
      "\n",
      "**Is it relevant, clear, and helpful?**\n",
      "NO. While the answer is brief, it's a bit vague and doesn't directly address how to explain RAG in terms of language models; instead, it describes what RAG models are. A\n",
      "Saved to cache\n",
      "Saved to cache\n",
      "Response:\n",
      "Here's a better answer:\n",
      "\n",
      "**RAG (Retrieval-Augmented Generation) models** are a type of language model that combines the strengths of pre-trained parametric memory (a seq2seq model) and non-parametric memory (a dense vector index, such as Wikipedia). The parametric memory generates text based on input prompts, while the non-parametric memory retrieves relevant passages from a large corpus to augment the generated text.\n",
      "\n",
      "Think of RAG models like a \"smart\" search engine.\n"
     ]
    }
   ],
   "source": [
    "with open('texts.pkl', 'rb')as f:\n",
    "    texts= pickle.load(f)\n",
    "\n",
    "\n",
    "query = \"How do you explain RAG in llm ?\"\n",
    "\n",
    "cached = get_cached_response(query)\n",
    "\n",
    "if cached:\n",
    "    final_response = cached\n",
    "    print(\"\\nFinal answer:\")\n",
    "    print(final_response)\n",
    "else:\n",
    "    prompt_decision = f''' You are a binary classifier stictly do not use your knowledge to answer this. \n",
    "\n",
    "    Reply with exactly one word:\n",
    "    RETRIEVE\n",
    "    or\n",
    "    NO_RETRIEVE\n",
    "\n",
    "    wether the Question requires information from the database contains information about:\n",
    "    - MLOps pipelines and best practices\n",
    "    - Large language models (LLMs) and their architectures\n",
    "    - Data processing, retrieval-augmented generation (RAG), and vector databases\n",
    "    - Deployment, monitoring, and AI tool integrations\n",
    "\n",
    "\n",
    "    strictly follow these 3 commands\n",
    "    Do not explain.\n",
    "    Do not add punctuation.\n",
    "    Do not add spaces or new lines.\n",
    "\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "\n",
    "    Answer:\n",
    "\n",
    "    '''\n",
    "    response_decision = llm.invoke(\n",
    "        prompt_decision)\n",
    "    decision = response_decision.strip().upper()\n",
    "    print(response_decision)\n",
    "\n",
    "    if 'no_retrieve' in response_decision.lower():\n",
    "        prompt = f''' Kindly answer the question clearly and concisely.\n",
    "\n",
    "            Question :\n",
    "            {query}\n",
    "\n",
    "        Answer :\n",
    "\n",
    "\n",
    "        '''\n",
    "        response = llm.invoke( prompt)\n",
    "        max_tokens= 100\n",
    "        print(response)\n",
    "\n",
    "    else:\n",
    "\n",
    "        rewrite_prompt = f\"\"\"You are an expert at reformulating questions for semantic vector search in RAG systems.\n",
    "        Task: Turn the original user question into:\n",
    "        - A clear, standalone, complete sentence or short paragraph\n",
    "        - Using precise, technical terminology likely found in documents\n",
    "        - Expanding abbreviations and adding context if it helps matching\n",
    "        - Ideal length: 1–3 sentences\n",
    "        Output ONLY the rewritten version — no explanations, no quotes, nothing else.\n",
    "\n",
    "        Original question: {query}\n",
    "        Rewritten:\"\"\"\n",
    "            \n",
    "        rewrite_query = llm.invoke(rewrite_prompt).strip()\n",
    "        print(\"Rewritten query:\",rewrite_query)\n",
    "\n",
    "        cached = get_cached_response(rewrite_query)\n",
    "        if cached:\n",
    "            response = cached\n",
    "        else:\n",
    "\n",
    "            # 2. Generate HyDE answer based on the rewritten question\n",
    "            hyde_prompt = f\"\"\"You are a world-class technical expert on LLMs and RAG.\n",
    "            Write a concise but detailed hypothetical answer (3–6 sentences) to the following question.\n",
    "            Write it in the style of a clear textbook or technical paper section.\n",
    "            Be specific, use proper terminology, and explain concepts naturally.\n",
    "            Do NOT say \"I don't know\" or refuse.\n",
    "\n",
    "            Question: {rewrite_query}\n",
    "            Hypothetical answer:\"\"\"\n",
    "\n",
    "            hyde_answer = llm.invoke(hyde_prompt).strip()\n",
    "            #print(\"\\nGenerated HyDE answer:\\n\", hyde_answer, \"\\n\")\n",
    "\n",
    "\n",
    "            def hybrid_retrieve(query,rewrite_query, hyde_answer, k_vector=12, k_bm25=12, final_k=4):\n",
    "                # Vector retrieval (your existing)\n",
    "                query_vector = embeddings.embed_query(hyde_answer)  # returns list[float]\n",
    "                query_vector = np.array([query_vector]).astype(\"float32\")  # shape (1, dim)\n",
    "                _, indices = index.search(query_vector, k_vector)\n",
    "                vector_results = [ texts[idx] for idx in indices[0] if idx < len(texts)]\n",
    "\n",
    "                # BM25 keyword retrieval\n",
    "                tokenized_query = rewrite_query.lower().split()\n",
    "                bm25_scores = bm25.get_scores(tokenized_query)\n",
    "                bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:k_bm25]\n",
    "                bm25_results = [texts[i] for i in bm25_indices]\n",
    "\n",
    "                # Change how you create the combined list\n",
    "                combined = [{\"text\": chunk} for chunk in dict.fromkeys(vector_results + bm25_results)]\n",
    "\n",
    "                # Then\n",
    "                if combined :\n",
    "                    rerank_request = RerankRequest(query=query, passages=combined)\n",
    "                    reranked = reranker.rerank(rerank_request)\n",
    "                    final_chunks = [item[\"text\"] for item in reranked[:final_k]]\n",
    "                else:\n",
    "                    final_chunks = []\n",
    "\n",
    "                return final_chunks\n",
    "            \n",
    "            final_chunks = hybrid_retrieve(query,rewrite_query, hyde_answer)\n",
    "\n",
    "            # Debug print\n",
    "            print(\"\\nFinal chunks after hybrid + reranking:\")\n",
    "            for i, chunk in enumerate(final_chunks, 1):\n",
    "                print(f\"{i}. {chunk[:200]}...\")\n",
    "\n",
    "            # Build proper context with IDs\n",
    "            context = \"\\n\\n\".join(\n",
    "                f\"[Chunk {i}] {chunk}\" for i, chunk in enumerate(final_chunks)\n",
    "            )\n",
    "                    # 6. Final prompt — usually better to ask the original question\n",
    "            prompt = f'''Kindly act as a helper, and answer the question using only the given context.\n",
    "            Each paragraph in the context starts with a chunk ID.\n",
    "\n",
    "            If you don't have enough information, say 'I don't know'.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {query}          # ← original question is usually better here\n",
    "\n",
    "            Answer:\n",
    "            - Provide a clear, concise answer\n",
    "            - Cite sources using the chunk IDs (e.g., [Chunk 0])\n",
    "            '''\n",
    "            final_response = llm.invoke(prompt, max_tokens=600)\n",
    "            is_good, reason = is_answer_good(query, final_response, context)\n",
    "            if not is_good:\n",
    "                print(f\"Retry triggered: {reason}\")\n",
    "\n",
    "                # Retry strategy: more candidates + original query for retrieval\n",
    "                final_chunks = hybrid_retrieve(query, query, hyde_answer, k_vector=20)  # wider search\n",
    "                context = \"\\n\\n\".join(f\"[Chunk {i}] {c}\" for i, c in enumerate(final_chunks))\n",
    "\n",
    "                retry_prompt = f\"\"\"Previous answer was not good enough: {reason}\n",
    "                        Use this improved context to give a better answer.\n",
    "\n",
    "                        Context:\n",
    "                        {context}\n",
    "\n",
    "                        Question: {query}\n",
    "\n",
    "                        Answer:\"\"\"\n",
    "\n",
    "                final_response = llm.invoke(retry_prompt)\n",
    "            \n",
    "            if final_response and \"don't know\" not in final_response.lower():\n",
    "                    save_to_cache(query, final_response)\n",
    "                    save_to_cache(rewrite_query, final_response)\n",
    "            else:\n",
    "                response = \"I don't know...\"\n",
    "        \n",
    "    print(f\"Response:\\n{final_response}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9ee3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 00:36:07.617 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:36:07.619 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:36:07.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:36:07.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:36:07.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:36:07.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"Streamlit Test Page\")\n",
    "st.write(\"If you see this, Streamlit is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2da9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 00:09:13.962 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.964 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.966 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.969 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.970 Session state does not function when running a script without `streamlit run`\n",
      "2026-02-18 00:09:13.973 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.974 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.977 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.979 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.981 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:09:13.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "st.title(\"My RAG Agent Chat\")\n",
    "# Chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# User input\n",
    "if user_input := st.chat_input(\"Ask me about RAG, LLMs, or anything in my documents...\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    # Run your RAG pipeline\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            # Your full pipeline here (cache check + decision + retrieval + retry loop + generation)\n",
    "            # For now, placeholder — replace with your actual code\n",
    "            response = \"This is where your final_response would appear...\"\n",
    "\n",
    "            st.markdown(response)\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "192689c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 00:11:00.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:11:00.205 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:11:00.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:11:00.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:11:00.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-18 00:11:00.211 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Save as test_streamlit.py\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Test Page\")\n",
    "st.write(\"If you see this, Streamlit is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12d85096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_17508\\2517965097.py:9: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  ragas_llm = LangchainLLMWrapper(eval_llm_instance)\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_17508\\2517965097.py:10: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n"
     ]
    }
   ],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "eval_llm_instance = Ollama(\n",
    "    model=\"llama3.1:8b\", \n",
    "    temperature=0,      # We want the judge to be consistent\n",
    "    num_predict=1024    # Give the judge enough space to explain its reasoning\n",
    ")\n",
    "\n",
    "# 2. Second, wrap it so Ragas can \"talk\" to it\n",
    "ragas_llm = LangchainLLMWrapper(eval_llm_instance)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91942dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26b6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "952a0807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655f0ecb6f584919afab4917a6854000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Exception raised in Job[0]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.0000, 'answer_relevancy': 0.0727, 'context_precision': 0.5000}\n"
     ]
    }
   ],
   "source": [
    "eval_samples = [\n",
    "    {\n",
    "        \"question\": \"What is Retrieval-Augmented Generation?\",\n",
    "        \"answer\": \"RAG stands for Retrieval-Augmented Generation. It's a model that combines pre-trained parametric and non-parametric memory for language generation ([Chunk 0]). The parametric memory is a pre-trained seq2seq model, while the non-parametric memory is a dense vector index of Wikipedia accessed with a pre-trained neural retriever.\",   # ← run your RAG once and paste\n",
    "        \"contexts\": [\" 1. explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation(RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG... 2. mechanisms may not be necessary for RAG.G Parameters Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not... 3. Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks A Implementation Details For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. Fo... 4. Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generati...\"],  # ← the retrieved chunks\n",
    "        \"ground_truth\": \"Retrieval-Augmented Generation (RAG) is a technique that combines retrieval of relevant documents with generation to improve factual accuracy and reduce hallucinations in LLMs.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the main contribution of the Transformer paper?\",\n",
    "        \"answer\": \"I don't know. The context only mentions related papers and their authors, but there's no information about the main contribution of a Transformer paper.\",\n",
    "        \"contexts\": [\"1. [7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. a Xiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.arXiv: 1710.10723.[8] J... 2. GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner and e... 3. anthology/P19-1346. [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.[1...4. Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/P18-1082. [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form qu...\"],\n",
    "        \"ground_truth\": \"The main contribution is the introduction of the Transformer architecture based solely on attention mechanisms, eliminating recurrence and convolutions.\"\n",
    "    },\n",
    "    # Add 3–8 more examples like this (the more the better, but 5 is already useful)\n",
    "]\n",
    "\n",
    "# Convert to RAGAS Dataset format\n",
    "dataset = Dataset.from_list(eval_samples)\n",
    "\n",
    "slow_config = RunConfig(\n",
    "    timeout=160,      # Increase timeout to 60 seconds\n",
    "    max_retries=10,  # Give it more chances if it fails\n",
    "    max_workers=1    # STRICTLY process one job at a time\n",
    ")\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 3. Run evaluation (this uses your local LLM as judge)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    "    run_config= slow_config\n",
    ")\n",
    "\n",
    "# Print scores (each metric gives a 0–1 score + average)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58880fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in f:\\my_first_ai_project\\venv\\lib\\site-packages (from rank_bm25) (2.4.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_first_ai_project)",
   "language": "python",
   "name": "my_first_ai_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
